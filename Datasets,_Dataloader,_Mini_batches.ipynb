{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahriyegrl/ds-677-deep-learning/blob/main/Datasets%2C_Dataloader%2C_Mini_batches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aotcAzv1OO"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQWEHWbWM0ZQ"
      },
      "source": [
        "### **Create data in NumPy**\n",
        "\n",
        "Each point comes from the 'ideal' line $y = 2x +1$, but adding some random small number. The goal will be to find what line fits the data as well as possible. And that line should be very close to that ideal line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsIRmSAnMc1u"
      },
      "source": [
        "# Data Generation\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(100, 1)\n",
        "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
        "\n",
        "# Shuffles the indices\n",
        "idx = np.arange(100)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# Uses first 80 random indices for train\n",
        "train_idx = idx[:80]\n",
        "# Uses the remaining indices for validation\n",
        "val_idx = idx[80:]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHhUGVDtv-_b"
      },
      "source": [
        "### **The last model we created**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTyYwLWZvaRK"
      },
      "source": [
        "class LayerLinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
        "        # this implicitly defines both a (the weight) and b (the intercept)\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now it only takes a call to the layer to make predictions\n",
        "        return self.linear(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCIShPzf1xSH"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# let's instantiate a model\n",
        "model = LayerLinearRegression().to(device)\n",
        "\n",
        "# define loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_2-jBv0vxGx"
      },
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Builds function that performs a step in the train loop\n",
        "    def train_step(x, y):\n",
        "        # Sets model to TRAIN mode\n",
        "        model.train()\n",
        "        # Makes predictions\n",
        "        yhat = model(x)\n",
        "        # Computes loss\n",
        "        loss = loss_fn(y, yhat)\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    # Returns the function that will be called inside the train loop\n",
        "    return train_step\n",
        "\n",
        "# Creates the train_step function for our model, loss function and optimizer\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "n_epochs = 1000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7DjLy2vwd8Z"
      },
      "source": [
        "### **The Dataset Class**\n",
        "\n",
        "We will now see how to handle bigger data and do gradient descent and minibatch-based optimization in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUrDb_HIwhvp",
        "outputId": "300852ab-e7a7-4ed4-fcec-a95aecd53df7"
      },
      "source": [
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "# Here we subclass the class Dataset that allows manipulation of data\n",
        "#\n",
        "# __init__, __getitem__, __len__ are basic methods that need to be implemented\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_tensor, y_tensor):\n",
        "        self.x = x_tensor\n",
        "        self.y = y_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMruXnKyyYw5",
        "outputId": "25c4442a-12a5-49e5-af94-85c2baad8772"
      },
      "source": [
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "# All of the above allows us to do very generic datasets\n",
        "# But the above is very common, so there is a pre-made way to do the same\n",
        "#\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyeWnRGUy8MO"
      },
      "source": [
        "**Important** Unlike what we did before, here we do not send the training tensors to the GPU. That is because GPU memory cannot fit the data if it is too big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1O0EdK-0KKw"
      },
      "source": [
        "### **The Dataloader**\n",
        "\n",
        "The Dataloader takes as input a dataset that we created earlier, a batch size and also whether we should shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpuZi740Ign"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjrXhbjR05hU"
      },
      "source": [
        "Here is how the train_loader can be used to make a training loop for minibatches. The inner loop grabs minibatches from the train loader,\n",
        "passes to the device where the model is, and just does a training step. It also computes the loss and keeps it around, so we can observe it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jghiqCir1LCC",
        "outputId": "e67e04c8-bb04-4497-d858-0152b018bec9"
      },
      "source": [
        "losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)  # this is the training function we defined earlier\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
        "        # therefore, we need to send those mini-batches to the\n",
        "        # device where the model \"lives\"\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "print(model.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[1.9695]])), ('linear.bias', tensor([1.0247]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJu9uK-1KA2"
      },
      "source": [
        "### **Finally: How to do validation/evaluation**\n",
        "\n",
        "First, we need to split the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp4saU144_ZU"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "\n",
        "## random_split applies on an object of the Dataset class\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "\n",
        "## then we build two different data loaders for training and validation\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KX0XpFv5a7j"
      },
      "source": [
        "This is the updated training loop, that now includes validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5GKRPxu5aZI",
        "outputId": "7ec31028-b784-4b3b-fd17-79e87995db6c"
      },
      "source": [
        "losses = []\n",
        "val_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # torch no_grad makes sure that the nested-below computations happen without gradients,\n",
        "    # since these are not needed for evaluation\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(y_val, yhat)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "print(model.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[1.9641]])), ('linear.bias', tensor([1.0084]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl8jAt5i5FsF",
        "outputId": "19201dc1-d0db-44b9-c6cb-fa42b0f7ea20"
      },
      "source": [
        "val_losses[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.010128086432814598"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRxEimxr5z6s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}